{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping 101 (30 minute read)\n",
    "\n",
    "## Goals of this tutorial:\n",
    "- How to Scrape Data.\n",
    "- Installing and Using Selenium.\n",
    "- How to use Selenium to dynamically interact with websites.\n",
    "- How to scrape useful information with BeautifulSoup.\n",
    "- How to moderate use and potential future issues.\n",
    "\n",
    "\n",
    "For this tutorial, I'll be running through a couple scripts I wrote to scrape hotel reviews off TripAdvisor.<br>\n",
    "I'll include those as .py files and would recommend running them in Spyder/some IDE. <br>\n",
    "Usually some unanticipated errors will occur midway through a session, and it's useful to rerun sections of a script, instead of trying to recall everything again from the command line.<br>\n",
    "\n",
    "<b>Disclaimer: </b>A lot of the methods and advice here are trial and error. Some of the potential issues at the end of this tutorial are ones I haven't encountered yet, but I've seen pop up in other tutorials. Feel more than welcome to add or correct aspects of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to Scrape Data\n",
    "\n",
    "If data is not available through a vendor, but is publicly available on the internet - it's very likely it can be scraped with minimal effort if the hosting site doesn't have an API.\n",
    "\n",
    "Here's a rough checklist of things to consider for a dataset spread between multiple pages on a site:\n",
    "\n",
    "1. Is there a pattern for how each instance of the data is stored/located on each page?<br>\n",
    "2. Is there a pattern for the URL of each page that corresponds to each instance?<br>\n",
    "3. Can I sample enough of the data in less than 100,000 page accesses?<br>\n",
    "\n",
    "\n",
    "<b>Ideally</b> all these answers would be yes. It would be great if the URLs have a pattern so that they could be easily generated by a function.<br>\n",
    "For instance, for an old WeatherUnderground scraper, historical weather data could be accessed by substituting in the day, month, and year into the URL format.</br>\n",
    "\n",
    "Simple example: try visiting </br>\n",
    "https://www.wunderground.com/history/airport/KSFO/2017/1/1/DailyHistory.html<br>\n",
    "https://www.wunderground.com/history/airport/KSFO/2016/1/1/DailyHistory.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sites I've seen are not like this, they use some <b>unique</b> identifiers to refer to distinct locations, dates, geographies, etc.<br>\n",
    "\n",
    "\n",
    "For instance, with TripAdvisor try these three URLs:\n",
    "https://www.tripadvisor.com/Hotels-g60898-Atlanta_Georgia-Hotels.html\n",
    "https://www.tripadvisor.com/Hotels-g60898-New_York_New_York-Hotels.html\n",
    "https://www.tripadvisor.com/Hotel_Review-g60898-d244079-Reviews-Motel_6_Atlanta-Atlanta_Georgia.html\n",
    "\n",
    "\n",
    "Trying to modify the first URL to a new location actually redirects the browser back to the original location (Atlanta).<br>\n",
    "This is likely because the <b>g60898</b> part of the URL is a unique identifier for Atlanta.<br>\n",
    "Similarly the <b>d244079</b> is likely a unique key for this particular hotel, so there's no clear way to only scrap hotels or locations we're interested in without literally scraping the entire site.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <b>This is not the end of the world.</b> <br>\n",
    "\n",
    "So long as there is a way to interact with the website automatically, the site can be crawled and the URLs can be stored and subsequently scraped.<br>\n",
    "To do this we need to dynamically interact with the website (using <b>Selenium</b>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing and Testing Selenium for Chrome on Mac "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply:\n",
    "\n",
    "1. pip install selenium<br>\n",
    "2. brew install webdriver\n",
    "3. Add chromedriver to path through the instructions __[here](http://www.kenst.com/2015/03/installing-chromedriver-on-mac-osx/)__\n",
    "\n",
    "Then try the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.sparkbeyond.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a new Chrome window (screenshot below) that opens up SparkBeyond's website. \n",
    "\n",
    "If you get the error: <b>ChromeDriver executable needs to be available in the path</b>, try working through the instructions on the above site again. If not, ping me on Slack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to use Selenium to Dynamically Interact with Websites\n",
    "\n",
    "Let's continue with that TripAdvisor example. If we want all the Motel 6 locations in a certain state, we can perform a search query through the website and then crawl through the results to store all relevant URLs. Let's practice doing this in Selenium, because eventually we have to loop through all 50 states. <b>(We're searching by State because any query is capped after 990 Hotels, so the results from an entire United States search query are incomplete.)</b>\n",
    "<br>\n",
    "\n",
    "To do this, we'll iteratively:\n",
    "1. Think about how we would interact with a page.\n",
    "2. Right (or Control) clicking to <b>inspect</b> the element we would interact with in the page source code.\n",
    "3. Code the action in Selenium.\n",
    "4. Run the code and validate the action was performed in the browser.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.tripadvisor.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the page source below, it looks like we have to click on this element, to open a search page.\n",
    "![test](TA_Images/TripAdvisorSearch.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element_by_class_name(\"mag_glass_parent\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll type in queries to the two new fields that appeared.\n",
    "\n",
    "<b>Note sometimes elements are not present until a certain action is performed on a webpage. There's a wait function to hold until this element is present before continuing, but I've found it's generally easier to just use a sleep timer.</b>\n",
    "\n",
    "![test](TA_Images/TripAdvisorInputField.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "hotel = 'Motel 6'\n",
    "state = 'Texas'\n",
    "\n",
    "time.sleep(np.random.lognormal(0,0.5) + 0.2)\n",
    "\n",
    "#Type the hotel name\n",
    "search_field = driver.find_element_by_id('mainSearch')\n",
    "search_field.send_keys(hotel)\n",
    "time.sleep(np.random.lognormal(0,0.5) + 0.2)\n",
    "\n",
    "#Type the city name\n",
    "location_field = driver.find_element_by_id('GEO_SCOPED_SEARCH_INPUT')\n",
    "location_field.send_keys(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see both fields inputed in your browser as seen below:\n",
    "\n",
    "![test](TA_Images/TripAdvisorInputFieldDone.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element_by_id('SEARCH_BUTTON_CONTENT')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How to scrape useful information with BeautifulSoup\n",
    "\n",
    "So now we've made it to the review page:\n",
    "\n",
    "![test](TA_Images/TripAdvisorSearchResults.png \"Title\")\n",
    "\n",
    "<br>\n",
    "It's time to look into the page source and find the URLs that point to pages that contain information about each individual hotel.\n",
    "<br>\n",
    "\n",
    "We're looking for a <b>unique</b> tag that contains information about the URL for each hotel. We can see on the right of the screenshot that:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "div class=\"title\" onclick=\"ta.setEvtCookie('Search_Results_Page', 'POI_Name', 'LODGING', 0, '/Hotel_Review-g55856-d244353-Reviews-Motel_6_Ft_Stockton-Fort_Stockton_Texas.html?t=28964')\" span Motel 6 Ft. Stockton /span /div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corresponds to a xml tag that includes the URL to the review page. Using BeautifulSoup, we can easily parse the page source for any instances of this tag, and then parse each individual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.tripadvisor.com/Hotel_Review-g55856-d244353-Reviews-Motel_6_Ft_Stockton-Fort_Stockton_Texas.html',\n",
       " 'https://www.tripadvisor.com/Hotel_Review-g56056-d1177217-Reviews-Motel_6_Junction-Junction_Texas.html',\n",
       " 'https://www.tripadvisor.com/Hotel_Review-g55505-d2555911-Reviews-Motel_6_Boerne-Boerne_Texas.html',\n",
       " 'https://www.tripadvisor.com/Hotel_Review-g55863-d1528531-Reviews-Motel_6_Fredericksburg-Fredericksburg_Texas.html']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml') \n",
    "hotel_locations = soup.find_all('div', {'class' : 'title'})\n",
    "\n",
    "hotel_urls = []\n",
    "for location in hotel_locations:\n",
    "    hotel_urls.append(\"https://www.tripadvisor.com\" + location.get('onclick').split(',')[-1].strip().strip(\"'\")[:-10])\n",
    "\n",
    "hotel_urls[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "Webpages change all the time, for instance this used to be the previous tag for hotel pages a week before I wrote this notebook was:<br>\n",
    "\n",
    "hotel_locations = soup.find_all('a', href = re.compile('/Hotel_Review'))<br>\n",
    "\n",
    "This may change again by the time you view this notebook.<br>\n",
    "\n",
    "\n",
    "## Scrapping JSON and XML Information\n",
    "\n",
    "Let's continue and scrape all potentially relevant data from the hotel page. It's important to take a close look at the page source, because sometimes a lot of information is in a <b>really nice JSON dictionary that very easy to parse.</b>\n",
    "<br>\n",
    "Other information can be very easily scraped by identifying a unique xml tag for that particular field, like \"div class = highlightedAmenity detailListItem\", and then using .find_all to grab all instances of that information. \n",
    "<br><b>BeautifulSoup</b> has several methods (like .next, .string) that can be chained to eventually get to the information you need. It'll look a bit ugly, but sites update sometimes frequently and quick and dirty might be the way to go.\n",
    "\n",
    "\n",
    "![test](TA_Images/TripAdvisorJSONDict.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('city', 'Abilene'),\n",
       "             ('state', 'Texas'),\n",
       "             ('zip_code', '79603-2305'),\n",
       "             ('address', '4951 W Stamford St'),\n",
       "             ('name', 'Motel 6 Abilene'),\n",
       "             ('price_range',\n",
       "              '$48 - $59 (Based on Average Rates for a Standard Room)'),\n",
       "             ('rating_value', '3.0'),\n",
       "             ('review_count', '66'),\n",
       "             ('review_topics', []),\n",
       "             ('hotel_amenities',\n",
       "              ['Wifi',\n",
       "               'Free Parking',\n",
       "               'Air Conditioning',\n",
       "               'Pool',\n",
       "               'Non-Smoking Rooms'])])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "hotel_page = np.random.choice(hotel_urls)\n",
    "driver.get(hotel_page)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "review_content = soup.find('script', type='application/ld+json')\n",
    "review_dict = json.loads(review_content.text)\n",
    "\n",
    "scrapped_amenities = soup.find_all('div', {'class': 'highlightedAmenity detailListItem'})\n",
    "hotel_amenities = []\n",
    "for amenity in scrapped_amenities:\n",
    "    hotel_amenities.append(amenity.next)\n",
    "\n",
    "review_topics = soup.find_all('span', {'class': 'ui_tagcloud fl'})\n",
    "hotel_topics = []\n",
    "for topic in review_topics:\n",
    "    hotel_topics.append(topic.next)\n",
    "\n",
    "hotel_info = OrderedDict(\n",
    "                 {'city'            : review_dict['address']['addressLocality'],\n",
    "                  'state'           : review_dict['address']['addressRegion'],\n",
    "                  'zip_code'        : review_dict['address']['postalCode'],\n",
    "                  'address'         : review_dict['address']['streetAddress'],\n",
    "                  'name'            : review_dict['name'],\n",
    "                  'price_range'     : review_dict['priceRange'],\n",
    "                  'rating_value'    : review_dict['aggregateRating']['ratingValue'],\n",
    "                  'review_count'    : review_dict['aggregateRating']['reviewCount'],\n",
    "                  'review_topics'   : hotel_topics,\n",
    "                  'hotel_amenities' : hotel_amenities,\n",
    "                  })\n",
    "\n",
    "hotel_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending to a File\n",
    "\n",
    "Now have a nice Ordered Dictionary, that can easily be appended to a file containing the scraped data using <b>csv.DictWriter</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Moderating Use and Potential Future Issues\n",
    "\n",
    "### Moderating Use\n",
    "So eventually a site can block multiple, repeated requests from the same user. To prevent this I would recommend:\n",
    "1. Sleep timers with lognormal distributions (preferably something like np.random.lognormal(1,0.5) + 2) as a starting point as timer between each action.\n",
    "2. Using a VPN - there are many free private VPN services (such as this one [here](https://www.tunnelbear.com/))\n",
    "3. Rewriting to a file after each page scrape to prevent losing significant amounts of data if there's an error or disconnection.\n",
    "\n",
    "### Potential Future Issues\n",
    "1. IP Ban (I believe using a VPN, especially one with built in IP cycling will prevent this.\n",
    "2. Sophisicated scraping detection tools (Distill Networks) that automatically detect Selenium - potential fix [here.](https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver)\n",
    "3. Complicated Javascript - apparently sometimes it's much easier to access the mobile site and scrape from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Future Directions\n",
    "\n",
    "Eventually I'll add the scripts that should give a good template to modify from and write your own scraper within a couple hours.<br>\n",
    "An easy way to build a scraper is:<br>\n",
    "- Identify endpoint URLs that contain relevant information\n",
    "- Write a script for the endpoint URL\n",
    "- Determine a search result page that will contain all endpoint URL Locations\n",
    "- Write a script to crawl through the search page and scrape URLs\n",
    "- Initialize a file and append URLs to the file\n",
    "- Load the URLs into a dataframe and iteratively scrape information from each endpoint URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
